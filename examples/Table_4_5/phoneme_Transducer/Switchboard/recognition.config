# -------- logging --------
[*]
configuration.channel    = output-channel
dot.channel              = nil
encoding                 = UTF-8
error.channel            = output-channel, stderr
log.channel              = output-channel
progress.channel         = output-channel
real-time-factor.channel = output-channel
statistics.channel       = output-channel
system-info.channel      = output-channel
time.channel             = output-channel
version.channel          = output-channel
warning.channel          = output-channel, stderr

[*.output-channel]
append     = no
compressed = no
file       = $(LOGFILE)
unbuffered = yes


# -------- global setup: data corpus, lexicon, etc. --------
[flf-lattice-tool.corpus]
audio-dir                      = /
capitalize-transcriptions      = no
file                           = /u/corpora/speech/hub5e_00/xml/hub5e_00.corpus.gz
progress-indication            = global
warn-about-unexpected-elements = yes

[flf-lattice-tool.global-cache]
file = global.cache

[flf-lattice-tool.lexicon]
file                    = /u/zhou/asr-exps/swb1/dependencies/monophone-eow/no-ctx/lexicon.xml
normalize-pronunciation = no


# -------- complete processing flow network -------- #
[flf-lattice-tool.network]
initial-nodes = segment

[flf-lattice-tool.network.segment]
links = 1->recognizer:1 0->archive-writer:1
type  = speech-segment

[flf-lattice-tool.network.archive-writer]
format = flf
info   = yes
links  = sink:1
path   = lattice.cache.$(TASK)
type   = archive-writer

[flf-lattice-tool.network.sink]
error-on-empty-lattice = no
type                   = sink
warn-on-empty-lattice  = yes


# -------- recognizer --------
[flf-lattice-tool.network.recognizer]
type                          = recognizer
links                         = archive-writer
use-mixture                   = no
pronunciation-scale           = 1.0
search-type                   = generic-seq2seq-tree-search

# ---- pre-computed posterior scores ----
[flf-lattice-tool.network.recognizer.feature-extraction]
file = feature.flow

[flf-lattice-tool.network.recognizer.feature-extraction.tf-fwd.input-map.info-0]
param-name             = features
seq-length-tensor-name = extern_data/placeholders/data/data_dim0_size
tensor-name            = extern_data/placeholders/data/data

[flf-lattice-tool.network.recognizer.feature-extraction.tf-fwd.output-map.info-0]
param-name  = log-posteriors
tensor-name = output_precompute/output_batch_major

# Note: model graph modification (forward once only)
# - feedback all |V| contexts and obtain |V|^2 scores for all frames
# - ILM (0.1) included
# also possible to use tf-ffnn-transducer to feed context on-the-fly 
# - slower but more variants investigation possible
# - embedding lookup still possible to speed-up
[flf-lattice-tool.network.recognizer.feature-extraction.tf-fwd.loader]
meta-graph-file    = /u/zhou/asr-exps/swb1/2022-06-24_speaker/work/crnn/compile/CompileTFGraphJob.qJUkQ4VITw87/output/graph.meta
saved-model-file   = /u/zhou/asr-exps/swb1/2022-06-24_speaker/work/crnn/custom_sprint_training/CustomCRNNSprintTrainingJob.5RM6VF4OuaSI/output/models/epoch.003 # MBR
type               = meta

# ---- S2S label scorer ----
[flf-lattice-tool.network.recognizer.label-scorer]
blank-label-index = 0
first-order       = yes
label-file        = /u/zhou/asr-exps/swb1/dependencies/monophone-eow/no-ctx/vocab
label-scorer-type = precomputed-log-posterior
scale             = 1.0
start-label-index = 0
use-start-label   = yes

# ---- LM ----
[flf-lattice-tool.network.recognizer.lm]
max-batch-size          = 128
scale                   = 1.0
transform-output-negate = yes
type                    = simple-transformer
vocab-file              = /u/zhou/asr-exps/swb1/dependencies/kazuki-transformer-lm-asru2019/vocabulary
vocab-unknown-word      = <unk>

[flf-lattice-tool.network.recognizer.lm.input-map.info-0]
param-name             = word
seq-length-tensor-name = extern_data/placeholders/delayed/delayed_dim0_size
tensor-name            = extern_data/placeholders/delayed/delayed

[flf-lattice-tool.network.recognizer.lm.output-map.info-0]
param-name  = softmax
tensor-name = output/output_batch_major

[flf-lattice-tool.network.recognizer.lm.loader]
meta-graph-file  = /u/zhou/asr-exps/swb1/dependencies/kazuki-transformer-lm-asru2019/inference.meta
saved-model-file = /u/zhou/asr-exps/swb1/dependencies/kazuki-transformer-lm-asru2019/network.020
type             = meta

# ---- S2S Tree Search ----
[flf-lattice-tool.network.recognizer.recognizer]
allow-blank-label            = yes
allow-label-recombination    = yes
label-recombination-limit    = 1
allow-word-end-recombination = yes
create-lattice               = yes
optimize-lattice             = no

label-pruning                = 18.0  # score pruning: e.g. lm.scale * 18
label-pruning-limit          = 20000 # histogram pruning: just upper limit
word-end-pruning             = 0.8   # 0.8 * label-pruning
word-end-pruning-limit       = 2000

full-sum-decoding            = yes # full-sum recombination: no for Viterbi recombination
lm-lookahead                 = yes
separate-lookahead-lm        = yes

[flf-lattice-tool.network.recognizer.recognizer.label-tree]
label-unit   = phoneme
skip-silence = yes

# bigram LM lookahead
[flf-lattice-tool.network.recognizer.recognizer.lm-lookahead]
cache-size-high = 3000
cache-size-low  = 2000
history-limit   = 1
scale           = 0.5

[flf-lattice-tool.network.recognizer.recognizer.lookahead-lm]
file  = /u/zhou/asr-exps/swb1/dependencies/zoltan_4gram.gz
image = /u/zhou/asr-exps/swb1/dependencies/monophone-eow/no-ctx/zoltan_4gram.image
scale = 1.0
type  = ARPA

