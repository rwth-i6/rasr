# -------- logging --------
[*]
configuration.channel    = output-channel
dot.channel              = nil
encoding                 = UTF-8
error.channel            = output-channel, stderr
log.channel              = output-channel
progress.channel         = output-channel
real-time-factor.channel = output-channel
statistics.channel       = output-channel
system-info.channel      = output-channel
time.channel             = output-channel
version.channel          = output-channel
warning.channel          = output-channel, stderr

[*.output-channel]
append     = no
compressed = no
file       = $(LOGFILE)
unbuffered = yes


# -------- global setup: data corpus (possible split of segments), lexicon, etc. --------
[flf-lattice-tool.corpus]
audio-dir                      = /
capitalize-transcriptions      = no
file                           = /u/corpora/speech/hub5e_00/xml/hub5e_00.corpus.gz
progress-indication            = global
segments.file                  = /u/zhou/asr-exps/swb1/2023-03-02_attention/work/corpus/SegmentCorpus.fOFLhnLn7Giw/output/segments.$(TASK)
warn-about-unexpected-elements = yes

[flf-lattice-tool.global-cache]
file = global.cache

[flf-lattice-tool.lexicon]
file                    = bpe.lexicon.xml.gz
normalize-pronunciation = no


# -------- complete processing flow network -------- #
[flf-lattice-tool.network]
initial-nodes = segment

[flf-lattice-tool.network.segment]
links = 1->recognizer:1 0->archive-writer:1
type  = speech-segment

[flf-lattice-tool.network.archive-writer]
format = flf
info   = yes
links  = sink:1
path   = lattice.cache.$(TASK)
type   = archive-writer

[flf-lattice-tool.network.sink]
error-on-empty-lattice = no
type                   = sink
warn-on-empty-lattice  = yes


# -------- recognizer --------
[flf-lattice-tool.network.recognizer]
type                          = recognizer
links                         = archive-writer
use-acoustic-model            = no
pronunciation-scale           = 1.0
feature-extraction.file       = feature.flow
lm.type                       = simple-history # no LM applied
search-type                   = generic-seq2seq-tree-search

# ---- S2S label scorer ----
[flf-lattice-tool.network.recognizer.label-scorer]
end-label-index         = 0
label-file              = /u/zhou/asr-exps/swb1/dependencies/mohammad_bpe_attention/vocab
label-scorer-type       = tf-attention
max-batch-size          = 128
reduction-factors       = 3,2
scale                   = 1.0
transform-output-negate = yes
use-start-label         = no

[flf-lattice-tool.network.recognizer.label-scorer.feature-input-map.info-0]
param-name             = feature
seq-length-tensor-name = extern_data/placeholders/data/data_dim0_size
tensor-name            = extern_data/placeholders/data/data

[flf-lattice-tool.network.recognizer.label-scorer.loader]
meta-graph-file    = /u/zhou/asr-exps/swb1/2023-03-02_attention/work/crnn/compile/CompileTFGraphJob.Dm59csJLIUv4/output/graph.meta
required-libraries = NativeLstm2.so:lstm_ops.so # compiled with matched TF version
saved-model-file   = /u/zhou/asr-exps/swb1/dependencies/mohammad_bpe_attention/epoch.198
type               = meta

# ---- S2S Tree Search ----
[flf-lattice-tool.network.recognizer.recognizer]
allow-label-recombination    = no
allow-word-end-recombination = no
create-lattice               = no
optimize-lattice             = no
use-lm-score                 = no
lm-lookahead                 = no
simple-beam-search           = yes
fixed-beam-search            = yes # use word-end-pruning-limit as fixed beam size
label-pruning                = 0.0
label-pruning-limit          = 12
word-end-pruning             = 0.0
word-end-pruning-limit       = 12
length-normalization         = yes
normalize-label-only         = yes

# score-pruning within the same softmax output of S2S model (default off)
# local-label-pruning          = 4.0

[flf-lattice-tool.network.recognizer.recognizer.label-tree]
label-unit   = word # <orth> in lexicon
skip-silence = yes

